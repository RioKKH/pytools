### 8章　DQN

DQN: Q学習＋ニューラルネット＋経験再生＋ターゲットネットワーク

#### 8.1 Open AI Gym

##### 状態と観測

- 状態
  環境に関しての「完璧な記述(情報)」
  状態が分かれば、マルコフ決定仮定により次の状態と報酬の確率分布が完全に決定する
- 観測
  状態の「部分的な記述」
  エージェントから問題(世界)の一部だけが見えるような問題
  タスクによっては状態と観測が一致する場合もあり得るが、一般的な強化学習では得られる情報は「観測」に相当する。

#### 8.2 DQNのコア技術

- Q学習はブートストラッピング
  推定値を使って推定値を更新する
  正確ではない推定値をもちいて推定値を更新するので、Q学習(TD法)は不安定になりやすい性質がある。
- 学習を安定させる技術
  - 経験再生  Experience Replay
  - ターゲットネットワーク Target Network

##### 8.2.1 経験再生 Experience Replay

- Q学習
  - エージェントが環境に対して行動を行う度にデータが生成される
  - 経験データ: $E_t = (S_t, A_t, R_t, S_{t+1})$をもちいてQ関数を更新する
  - 経験データ間には強い相関がある
    - $E_t$と$E_{t+1}$の間には強い相関がある
    - Q学習は偏りの或るデータを使って学習を進めていることになる。これはCNNでミニバッチでデータをランダムに選んで偏りの無いデータを用いて学習していたのと大きく異なる点。
  - 経験リプレイを導入してこの問題に対応する。
  - ただし、経験再生は**方策オフ型のアルゴリズムにしか使えない**点に注意すること。

##### 8.2.3 ターゲットネットワーク

- 教師有り学習：学習の途中でラベルが変わるような事はない。
  - MNISTの或る画像の正解ラベルが"4"だったらずっと"4"のまま
- Q学習
  $Q(S_t, A_t)$が$R_t + \gamma\text{max}_aQ(S_{t+1}, a)$（TDターゲット）となるようにQ関数を更新する。TDターゲットは教師有り学習における正解ラベルに相当する。TDターゲットの値は、Q関数が更新されると変動する。
  - これが教師学習とQ学習の違い
  - TDターゲットを固定するテクニックを用いる！→それがターゲットネットワーク
  - Q関数を表すネットワークを2つ導入する
    - 一つはオリジナルの関数: qnet
      - これは都度更新するためのネットワーク
    - 同じ構造のネットワーク: qnet_target
      - 間をあけて定期的にターゲットネットワークの更新を行う
      - qnet_targetを使ってTDターゲットの値をけいさんする。これによって教師ラベルであるTDターゲットの変動を抑えられる

#### 8.3 DQNとAtari

##### 8.3.2 前処理

Pongのようなゲームの場合、ある一つの時刻の(画像)情報だけからでは、ボールの進行方向が不明の為、最適な行動を選択することが出来ない。このような問題の場合には　POMDP 部分観測マルコフ決定仮定 Partialy Observable Markov Decision Process を用いる。

DQNの論文では４フレームの画像を一つの状態として重ね合わせて扱っている。これによって状態の遷移（ボールの移動が分かる）。これによって今まで通りMDPとして扱う事が出来る。

その他の前処理

- 画像の周囲のトリミング
- グレイスケール化
- 画像のリサイズ
- 画素の正規化（0.0から1.0の間に変換）

##### 8.3.3 CNN

Atariのような画像を入力データとして扱う場合には、CNNが有効。

##### 8.3.4 その他の工夫

- GPUの利用
- εの調整
  DQNの元論文では、最初の100万ステップは1.0から0.1まで線形に減少させ、それ以降はε= 0.1で固定する方式が採用されている。
- 報酬クリッピング (Reward Clipping)
  方針を-1.0 ~ 1.0の範囲に収まるように調整し、報酬のスケールをそろえることで、学習を円滑化している。

#### 8.4 DQNの拡張

##### 8.4.1 Double DQN

Q関数で用いるTDターゲット: $R_t + \gamma \text{max}_aQ_{\theta'}(S_{t+1}, a)$
Double DQNで用いるTD: $R_t + \gamma Q_{\theta'}(S_{t+1}, \text{argmax}_aQ_{\theta}(S_{t+1}, a))$
ポイントは、$Q_{\theta}(s, a)$をつかって最大となるように行動を選び、実際の値は$Q_{\theta'}(s, a)$から取得すること。２つのQ関数を使い分けることで、過大評価が解消され、学習が安定する。

##### 8.4.2 優先度付き経験再生

経験データをランダムに選ぶのではなくて、優先度に応じて選びやすくする。

##### 8.4.3 Dueling DQN

アドバンテージ関数 = Q関数 - 価値関数