### 8章　DQN

DQN: Q学習＋ニューラルネット＋経験再生＋ターゲットネットワーク

#### 8.1 Open AI Gym

##### 状態と観測

- 状態
  環境に関しての「完璧な記述(情報)」
  状態が分かれば、マルコフ決定仮定により次の状態と報酬の確率分布が完全に決定する
- 観測
  状態の「部分的な記述」
  エージェントから問題(世界)の一部だけが見えるような問題
  タスクによっては状態と観測が一致する場合もあり得るが、一般的な強化学習では得られる情報は「観測」に相当する。

#### 8.2 DQNのコア技術

- Q学習はブートストラッピング
  推定値を使って推定値を更新する
  正確ではない推定値をもちいて推定値を更新するので、Q学習(TD法)は不安定になりやすい性質がある。
- 学習を安定させる技術
  - 経験再生  Experience Replay
  - ターゲットネットワーク Target Network

##### 8.2.1 経験再生 Experience Replay

- Q学習
  - エージェントが環境に対して行動を行う度にデータが生成される
  - 経験データ: $E_t = (S_t, A_t, R_t, S_{t+1})$をもちいてQ関数を更新する
  - 経験データ間には強い相関がある
    - $E_t$と$E_{t+1}$の間には強い相関がある
    - Q学習は偏りの或るデータを使って学習を進めていることになる。これはCNNでミニバッチでデータをランダムに選んで偏りの無いデータを用いて学習していたのと大きく異なる点。
  - 経験リプレイを導入してこの問題に対応する。